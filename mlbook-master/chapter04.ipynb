{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トピックモデル入門\n",
    "## 潜在ディリクレ分配法（LDA）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "% matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "import lda\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents:  1000\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "with open(os.path.normpath('./dataset/document_word_data.json'), 'r') as f:\n",
    "    doc_data = json.load(f)\n",
    "\n",
    "all_doc_index = doc_data.keys()\n",
    "print('Total Documents: ', len(all_doc_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ママ, 子供, 健康, づくり, 新た, ライフスタイル, 提案, ママ, マルシェ, 府, 府, 市, さまざま, 家族, ら, 日, もん, 商品, ほか, ハロ, 子供, 商品, プレゼント, 各日, 人, 会場, 木, ぬくもり, 木, 子供, づくり, 会, 木, 日, 午後, 時, 今年度, 森林, 林業, 木材, 大使, ミス, 日本, みどり, 帆, 南, さん, 府, 木材, 会, 湯川, 昌子, さん, ら, 女性, 人, 参加, スギ, ヒノキ, 木材, 放出, 健康, 効果, 木材, こと, 女性, 入場, 無料, 文化, 園, 入園, 料, 大人, 円, 円'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データの確認\n",
    "# 715番目の文書に含まれる単語\n",
    "', '.join(doc_data['715'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocablary Number:  8709\n"
     ]
    }
   ],
   "source": [
    "# 全単語のリストを作成\n",
    "all_vocab = []\n",
    "for v in doc_data.values():\n",
    "    for vv in v:\n",
    "        all_vocab.append(vv)\n",
    "\n",
    "# 重複を消すためにsetしてlistにする\n",
    "all_vocab = list(set(all_vocab))\n",
    "vocab_num = len(all_vocab)\n",
    "print('Vocablary Number: ', vocab_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# スライシングして学習データとテスト用データに分けるため、リストをNumpyの配列にしておきます。\n",
    "all_doc_index_ar = np.array(list(all_doc_index))\n",
    "\n",
    "# 学習データのサンプル数を決める\n",
    "train_portion = 0.7\n",
    "train_num = int(len(all_doc_index_ar) * train_portion)\n",
    "\n",
    "# 学習データとテスト用データに分ける\n",
    "np.random.shuffle(all_doc_index_ar)\n",
    "train_doc_index = all_doc_index_ar[:train_num]\n",
    "test_doc_index = all_doc_index_ar[train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# スパース行列\n",
    "A_train = sparse.lil_matrix((len(train_doc_index), len(all_vocab)), dtype=np.int)\n",
    "A_test = sparse.lil_matrix((len(test_doc_index), len(all_vocab)), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_vocabのリストの中で、単語のインデックス番号を取得するためNumpy配列にしておく\n",
    "all_vocab_ar = np.array(all_vocab)\n",
    "train_doc_index_ar = np.array(train_doc_index)\n",
    "test_doc_index_ar = np.array(test_doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train total elements num : 34205\n",
      "Test total elements num : 13839\n"
     ]
    }
   ],
   "source": [
    "# 学習用\n",
    "train_total_elements_num = 0\n",
    "for i in range(len(train_doc_index)):\n",
    "    doc_idx = train_doc_index[i]\n",
    "    row_data = Counter(doc_data[doc_idx])\n",
    "    \n",
    "    for word in row_data.keys():\n",
    "        word_idx = np.where(all_vocab_ar == word)[0][0]\n",
    "        A_train[i, word_idx] = row_data[word]\n",
    "        train_total_elements_num += 1\n",
    "print('Train total elements num :', train_total_elements_num)\n",
    "\n",
    "\n",
    "# テスト用\n",
    "test_total_elements_num = 0\n",
    "for i in range(len(test_doc_index)):\n",
    "    doc_idx = test_doc_index[i]\n",
    "    row_data = Counter(doc_data[doc_idx])\n",
    "    \n",
    "    for word in row_data.keys():\n",
    "        word_idx = np.where(all_vocab_ar == word)[0][0]\n",
    "        A_test[i, word_idx] = row_data[word]\n",
    "        test_total_elements_num += 1\n",
    "print('Test total elements num :', test_total_elements_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CountVectorizerを用いてdoc_dataから簡単に疎行列を作ることもできます\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=lambda a: a, analyzer=lambda a: a)\n",
    "vectorizer.fit(doc_data[idx] for idx in all_doc_index)\n",
    "A_train = vectorizer.transform(doc_data[idx] for idx in train_doc_index)\n",
    "A_test = vectorizer.transform(doc_data[idx] for idx in test_doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 実際にLDAを適用してみよう (Scikit-learnを使った例）\n",
    "# n_topics: トピック数を指定\n",
    "model1 = LatentDirichletAllocation(n_topics=20,\n",
    "                                   doc_topic_prior=0.001,\n",
    "                                   topic_word_prior=0.5,\n",
    "                                   max_iter=5,\n",
    "                                   learning_method='online',\n",
    "                                   learning_offset=50.,\n",
    "                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=0.001,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_jobs=1, n_topics=20, perp_tol=0.1, random_state=0,\n",
       "             topic_word_prior=0.5, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(A_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize_components = model1.components_ / model1.components_.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "クラシック 最後 ぷぅ 光宏 ミニチュア プレゼン デカダンス 対極 ディスプレイ ドル リンクス 匙 景気 即効 オノ インフルエンザ 南ア テンポ 拡充 ステンレス\n",
      "\n",
      "Topic #1:\n",
      "マンション 勧告 ステインペン 吉原 スパゲティ 今宵 広樹 バングル ハロウィンマエストロ 本数 ラクオリア 橋 もみじ 油彩 サイセイシマス いくつ 授業 地 有無 フラン\n",
      "\n",
      "Topic #2:\n",
      "法律 ハコスコ 全勝 なほ とうさん 昆布 浅口 全治 テクニカル 太宰府天満宮 内山 年収 カウント ェクト 使い手 いちど ノベルティ メイワサンピア 外伝 支所\n",
      "\n",
      "Topic #3:\n",
      "後日 いろいろ メディカル シトロン クロエ 実機 千尋 ハベル 入場 旗 冷 ワタシプラス 名実 ジグ ガ スロアプリ おと なめらか 別 灰\n",
      "\n",
      "Topic #4:\n",
      "ハンデ 損失 恩 桐 橋本 カニサレス 正明 ラヴクラフトクトゥルフ テレコム 入力 付け シット アニメ せっさたくま 円建て フィッシング 心境 当たり年 時差 加入\n",
      "\n",
      "Topic #5:\n",
      "振替 カゲ 幕張 ニコン 東宝 スキャンダル 所在地 全国 区分 幼なじみ ディスプレイ ベスト ストック 昨今 学修 ゲリラ リンクトイン 各地 ディスク サンダル\n",
      "\n",
      "Topic #6:\n",
      "涼子 岡本 激戦 凡打 岡 ウィンズ ツッコミ 林業 実業 エグジビション 日帰り プラプラ ブックレット カワイイ ラジオ 善悪 東山 出店 もち ヴァルヴァル\n",
      "\n",
      "Topic #7:\n",
      "召使い リツトル 匠 ベティ おとな 慎吾 判決 俵山 かげん 勘 滝沢 夫婦 廃棄 味わい ざき 期 チャネル プラザ アントニオ 岳\n",
      "\n",
      "Topic #8:\n",
      "周 どっち ライフ ヒョウ カスタマイズ ニラ 方位 亡国 松永 ダルマ 気泡 川澄 困惑 温泉 ラスト 才女 ステッチ 子ども こちら のこぎり\n",
      "\n",
      "Topic #9:\n",
      "ピアス 東シナ海 全力 としのり 元手 武 トリ 動機 包括 ゴジエヴァコラボ 岩国 同僚 ワケ 嶋村 ミドル トラディショナル 消化 支援 為替 ニッポン\n",
      "\n",
      "Topic #10:\n",
      "政党 マイナス 春奈 フィニッシュ 古巣 宮本 ソフトドリンク スペシャリスト 原因 引き取り手 作り方 ラテックス サウジアラビア 湖 故旧 オシドリ トライアウト 実力 イソフラボン チャンピオンシップ\n",
      "\n",
      "Topic #11:\n",
      "キス わし コップ カエデ 有効 モダン 人妻 急死 入園 ライブラリ 幸子 ホン とうさん 博士 同点 点字 仏像 傍ら 本屋 宿\n",
      "\n",
      "Topic #12:\n",
      "バエス ランキング 残念 可動 ハナ ベルダスコ 今村 暇潰し ミ まいこ 佐和子 レストラン 屋台 売り切れ ハノイ パンチ 安打 イガリ どころ 同い年\n",
      "\n",
      "Topic #13:\n",
      "メイメイ タイアップ コレクション エコノミスト ナント い 島崎 おい 忠 多度津 ジェル 東大 テレビ局 べっぴん 地殻 フレ 核兵器 旋波 客室 厳密\n",
      "\n",
      "Topic #14:\n",
      "油 得意 テスラ ガッツ 敏感 権利 創価大 別居 本城 持ち寄り あずさ リンクス カレラ フレデリック チェコ センシング 定か ロス 大堀 施川\n",
      "\n",
      "Topic #15:\n",
      "ふく ロダン 宏幸 インパクト 喜八 セント 同名 尺 テキサス コンパウンディング ウェブストア コチュジャン 共 増大 サガン ドコモ べっち この先 対応 ツボ\n",
      "\n",
      "Topic #16:\n",
      "アレン 死地 オフィス 巡 最多 子浦 カッコ ハプニング 倍増 キッチン 松江 リポビタン 右側 ブリリアントレッド ベイベ 巡業 庭園 マインツ 殺伐 克\n",
      "\n",
      "Topic #17:\n",
      "がわり 大別 ブルボンヌママ フクス プラスチック フリット ひも トレインスポッティング 椎名 くじ引き 有数 アルティメイトオメガ 体型 本意 沼津 探査 デミオ スポニチ ロッソ キクチ\n",
      "\n",
      "Topic #18:\n",
      "ドイツ 引 そ ヒ 客殿 デマ 気掛かり エブリデイ 柳原 依拠 けんいち アップ ストレイドッグス 満塁 公正 小布施 日活 ズレ 塚沢 バックエクステンション\n",
      "\n",
      "Topic #19:\n",
      "六 フォトジェニック 住吉 戦災 おっぱい パンツ 人為 ちょこ 森林 ぷぅ ヘディング 因 サヌア 席巻 パンナコッタ ツムツム おまかせ タイチ アンサンブル 再戦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/applications/\n",
    "# topics_extraction_with_nmf_lda.html　より\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(normalize_components):\n",
    "    print('Topic #%d:' % topic_idx)\n",
    "    print(' '.join([all_vocab_ar[i] for i in\n",
    "                    topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.63880695e-05,   1.63880695e-05,   1.63880695e-05, ...,\n",
       "          1.63880695e-05,   1.63880695e-05,   1.63880695e-05],\n",
       "       [  1.58679784e-05,   1.58679784e-05,   1.58679784e-05, ...,\n",
       "          1.58679784e-05,   1.58679784e-05,   1.58679784e-05],\n",
       "       [  8.31946755e-05,   8.31946755e-05,   8.31946755e-05, ...,\n",
       "          8.31946755e-05,   8.31946755e-05,   8.31946755e-05],\n",
       "       ..., \n",
       "       [  1.13610543e-05,   1.13610543e-05,   1.13610543e-05, ...,\n",
       "          1.13610543e-05,   1.13610543e-05,   1.13610543e-05],\n",
       "       [  4.13188993e-06,   4.13188993e-06,   4.13188993e-06, ...,\n",
       "          4.13188993e-06,   4.13188993e-06,   4.13188993e-06],\n",
       "       [  5.64907920e-06,   5.64907920e-06,   5.64907920e-06, ...,\n",
       "          5.64907920e-06,   5.64907920e-06,   5.64907920e-06]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_data = model1.transform(A_train)\n",
    "doc_topic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize_doc_topic_data = \\\n",
    " doc_topic_data / doc_topic_data.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: probality: 0.000016\n",
      "Topic #1: probality: 0.000016\n",
      "Topic #2: probality: 0.000016\n",
      "Topic #3: probality: 0.999689\n",
      "Topic #4: probality: 0.000016\n",
      "Topic #5: probality: 0.000016\n",
      "Topic #6: probality: 0.000016\n",
      "Topic #7: probality: 0.000016\n",
      "Topic #8: probality: 0.000016\n",
      "Topic #9: probality: 0.000016\n",
      "Topic #10: probality: 0.000016\n",
      "Topic #11: probality: 0.000016\n",
      "Topic #12: probality: 0.000016\n",
      "Topic #13: probality: 0.000016\n",
      "Topic #14: probality: 0.000016\n",
      "Topic #15: probality: 0.000016\n",
      "Topic #16: probality: 0.000016\n",
      "Topic #17: probality: 0.000016\n",
      "Topic #18: probality: 0.000016\n",
      "Topic #19: probality: 0.000016\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, prob in enumerate(normalize_doc_topic_data[0]):\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "対数尤度:  -635671.056699\n",
      "Perplexity:  198431859624.0\n"
     ]
    }
   ],
   "source": [
    "loglikelihood = model1.score(A_test)\n",
    "ppl = model1.perplexity(A_test)\n",
    "print('対数尤度: ', loglikelihood)\n",
    "print('Perplexity: ', ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: probality: 0.000007\n",
      "Topic #1: probality: 0.000007\n",
      "Topic #2: probality: 0.000007\n",
      "Topic #3: probality: 0.681624\n",
      "Topic #4: probality: 0.000007\n",
      "Topic #5: probality: 0.000007\n",
      "Topic #6: probality: 0.000007\n",
      "Topic #7: probality: 0.000007\n",
      "Topic #8: probality: 0.000007\n",
      "Topic #9: probality: 0.000007\n",
      "Topic #10: probality: 0.000007\n",
      "Topic #11: probality: 0.318252\n",
      "Topic #12: probality: 0.000007\n",
      "Topic #13: probality: 0.000007\n",
      "Topic #14: probality: 0.000007\n",
      "Topic #15: probality: 0.000007\n",
      "Topic #16: probality: 0.000007\n",
      "Topic #17: probality: 0.000007\n",
      "Topic #18: probality: 0.000007\n",
      "Topic #19: probality: 0.000007\n"
     ]
    }
   ],
   "source": [
    "test_doc_topic_data = model1.transform(A_test)\n",
    "normalize_test_doc_topic_data = \\\n",
    " test_doc_topic_data / test_doc_topic_data.sum(axis=1, keepdims=True)\n",
    "for topic_idx, prob in enumerate(normalize_test_doc_topic_data[0]):\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 700\n",
      "INFO:lda:vocab_size: 8709\n",
      "INFO:lda:n_words: 61448\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 1500\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -701796\n",
      "INFO:lda:<10> log likelihood: -549960\n",
      "INFO:lda:<20> log likelihood: -525280\n",
      "INFO:lda:<30> log likelihood: -518245\n",
      "INFO:lda:<40> log likelihood: -515623\n",
      "INFO:lda:<50> log likelihood: -514603\n",
      "INFO:lda:<60> log likelihood: -514227\n",
      "INFO:lda:<70> log likelihood: -513453\n",
      "INFO:lda:<80> log likelihood: -511959\n",
      "INFO:lda:<90> log likelihood: -511978\n",
      "INFO:lda:<100> log likelihood: -510700\n",
      "INFO:lda:<110> log likelihood: -510176\n",
      "INFO:lda:<120> log likelihood: -510188\n",
      "INFO:lda:<130> log likelihood: -510407\n",
      "INFO:lda:<140> log likelihood: -509837\n",
      "INFO:lda:<150> log likelihood: -510330\n",
      "INFO:lda:<160> log likelihood: -509882\n",
      "INFO:lda:<170> log likelihood: -509924\n",
      "INFO:lda:<180> log likelihood: -510619\n",
      "INFO:lda:<190> log likelihood: -509971\n",
      "INFO:lda:<200> log likelihood: -509984\n",
      "INFO:lda:<210> log likelihood: -510469\n",
      "INFO:lda:<220> log likelihood: -509224\n",
      "INFO:lda:<230> log likelihood: -510370\n",
      "INFO:lda:<240> log likelihood: -510856\n",
      "INFO:lda:<250> log likelihood: -510139\n",
      "INFO:lda:<260> log likelihood: -510212\n",
      "INFO:lda:<270> log likelihood: -510352\n",
      "INFO:lda:<280> log likelihood: -509712\n",
      "INFO:lda:<290> log likelihood: -509992\n",
      "INFO:lda:<300> log likelihood: -509924\n",
      "INFO:lda:<310> log likelihood: -509815\n",
      "INFO:lda:<320> log likelihood: -509915\n",
      "INFO:lda:<330> log likelihood: -510037\n",
      "INFO:lda:<340> log likelihood: -510204\n",
      "INFO:lda:<350> log likelihood: -509308\n",
      "INFO:lda:<360> log likelihood: -510003\n",
      "INFO:lda:<370> log likelihood: -509615\n",
      "INFO:lda:<380> log likelihood: -509698\n",
      "INFO:lda:<390> log likelihood: -510322\n",
      "INFO:lda:<400> log likelihood: -509793\n",
      "INFO:lda:<410> log likelihood: -509317\n",
      "INFO:lda:<420> log likelihood: -509890\n",
      "INFO:lda:<430> log likelihood: -509995\n",
      "INFO:lda:<440> log likelihood: -509881\n",
      "INFO:lda:<450> log likelihood: -509451\n",
      "INFO:lda:<460> log likelihood: -509408\n",
      "INFO:lda:<470> log likelihood: -510062\n",
      "INFO:lda:<480> log likelihood: -510538\n",
      "INFO:lda:<490> log likelihood: -509765\n",
      "INFO:lda:<500> log likelihood: -509454\n",
      "INFO:lda:<510> log likelihood: -509533\n",
      "INFO:lda:<520> log likelihood: -509475\n",
      "INFO:lda:<530> log likelihood: -509300\n",
      "INFO:lda:<540> log likelihood: -509541\n",
      "INFO:lda:<550> log likelihood: -509615\n",
      "INFO:lda:<560> log likelihood: -510171\n",
      "INFO:lda:<570> log likelihood: -509093\n",
      "INFO:lda:<580> log likelihood: -509637\n",
      "INFO:lda:<590> log likelihood: -510518\n",
      "INFO:lda:<600> log likelihood: -509647\n",
      "INFO:lda:<610> log likelihood: -509994\n",
      "INFO:lda:<620> log likelihood: -508830\n",
      "INFO:lda:<630> log likelihood: -509728\n",
      "INFO:lda:<640> log likelihood: -510131\n",
      "INFO:lda:<650> log likelihood: -509740\n",
      "INFO:lda:<660> log likelihood: -510078\n",
      "INFO:lda:<670> log likelihood: -510064\n",
      "INFO:lda:<680> log likelihood: -509201\n",
      "INFO:lda:<690> log likelihood: -509264\n",
      "INFO:lda:<700> log likelihood: -509783\n",
      "INFO:lda:<710> log likelihood: -509364\n",
      "INFO:lda:<720> log likelihood: -510451\n",
      "INFO:lda:<730> log likelihood: -510035\n",
      "INFO:lda:<740> log likelihood: -509288\n",
      "INFO:lda:<750> log likelihood: -509863\n",
      "INFO:lda:<760> log likelihood: -510302\n",
      "INFO:lda:<770> log likelihood: -509762\n",
      "INFO:lda:<780> log likelihood: -508680\n",
      "INFO:lda:<790> log likelihood: -509966\n",
      "INFO:lda:<800> log likelihood: -509302\n",
      "INFO:lda:<810> log likelihood: -510033\n",
      "INFO:lda:<820> log likelihood: -509176\n",
      "INFO:lda:<830> log likelihood: -508990\n",
      "INFO:lda:<840> log likelihood: -509098\n",
      "INFO:lda:<850> log likelihood: -509640\n",
      "INFO:lda:<860> log likelihood: -509778\n",
      "INFO:lda:<870> log likelihood: -509996\n",
      "INFO:lda:<880> log likelihood: -509685\n",
      "INFO:lda:<890> log likelihood: -509429\n",
      "INFO:lda:<900> log likelihood: -509783\n",
      "INFO:lda:<910> log likelihood: -509598\n",
      "INFO:lda:<920> log likelihood: -509960\n",
      "INFO:lda:<930> log likelihood: -509956\n",
      "INFO:lda:<940> log likelihood: -509547\n",
      "INFO:lda:<950> log likelihood: -509098\n",
      "INFO:lda:<960> log likelihood: -510307\n",
      "INFO:lda:<970> log likelihood: -509600\n",
      "INFO:lda:<980> log likelihood: -508829\n",
      "INFO:lda:<990> log likelihood: -510269\n",
      "INFO:lda:<1000> log likelihood: -509798\n",
      "INFO:lda:<1010> log likelihood: -509754\n",
      "INFO:lda:<1020> log likelihood: -509596\n",
      "INFO:lda:<1030> log likelihood: -509843\n",
      "INFO:lda:<1040> log likelihood: -508818\n",
      "INFO:lda:<1050> log likelihood: -510393\n",
      "INFO:lda:<1060> log likelihood: -509429\n",
      "INFO:lda:<1070> log likelihood: -509676\n",
      "INFO:lda:<1080> log likelihood: -509197\n",
      "INFO:lda:<1090> log likelihood: -510331\n",
      "INFO:lda:<1100> log likelihood: -509546\n",
      "INFO:lda:<1110> log likelihood: -509412\n",
      "INFO:lda:<1120> log likelihood: -509828\n",
      "INFO:lda:<1130> log likelihood: -509236\n",
      "INFO:lda:<1140> log likelihood: -510203\n",
      "INFO:lda:<1150> log likelihood: -509404\n",
      "INFO:lda:<1160> log likelihood: -508960\n",
      "INFO:lda:<1170> log likelihood: -509478\n",
      "INFO:lda:<1180> log likelihood: -509692\n",
      "INFO:lda:<1190> log likelihood: -509205\n",
      "INFO:lda:<1200> log likelihood: -509052\n",
      "INFO:lda:<1210> log likelihood: -510236\n",
      "INFO:lda:<1220> log likelihood: -509543\n",
      "INFO:lda:<1230> log likelihood: -509791\n",
      "INFO:lda:<1240> log likelihood: -509496\n",
      "INFO:lda:<1250> log likelihood: -509540\n",
      "INFO:lda:<1260> log likelihood: -509702\n",
      "INFO:lda:<1270> log likelihood: -509472\n",
      "INFO:lda:<1280> log likelihood: -509663\n",
      "INFO:lda:<1290> log likelihood: -509258\n",
      "INFO:lda:<1300> log likelihood: -510303\n",
      "INFO:lda:<1310> log likelihood: -509529\n",
      "INFO:lda:<1320> log likelihood: -509071\n",
      "INFO:lda:<1330> log likelihood: -509771\n",
      "INFO:lda:<1340> log likelihood: -509169\n",
      "INFO:lda:<1350> log likelihood: -509298\n",
      "INFO:lda:<1360> log likelihood: -510378\n",
      "INFO:lda:<1370> log likelihood: -509704\n",
      "INFO:lda:<1380> log likelihood: -509184\n",
      "INFO:lda:<1390> log likelihood: -509883\n",
      "INFO:lda:<1400> log likelihood: -510013\n",
      "INFO:lda:<1410> log likelihood: -509910\n",
      "INFO:lda:<1420> log likelihood: -509791\n",
      "INFO:lda:<1430> log likelihood: -510456\n",
      "INFO:lda:<1440> log likelihood: -509940\n",
      "INFO:lda:<1450> log likelihood: -510172\n",
      "INFO:lda:<1460> log likelihood: -509047\n",
      "INFO:lda:<1470> log likelihood: -509498\n",
      "INFO:lda:<1480> log likelihood: -509271\n",
      "INFO:lda:<1490> log likelihood: -509755\n",
      "INFO:lda:<1499> log likelihood: -509753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "後日 プロダクション いろいろ 椎名 本意 木漏れ日 ラッピング キュレ タワマン キム コンシリウム 歯ぎしり マンコ 兵団 炉 ナイン キッズプログラム 尾崎 ガッツ そいつ\n",
      "\n",
      "Topic #1:\n",
      "名実 ウェイ 安佐北 パイオニア 交差 インド 出所 排尿 グラマラス 報告 冷 深夜 四季 洗い 嗜好 別れ ホコリセンサ 助っ人 国境 デバ\n",
      "\n",
      "Topic #2:\n",
      "激戦 さつまいも マギ 斜め 半日 エフエフ サステナブル 宇宙 バック 忍 お勤め アラン 板ずり 土人 てらそま フラッシュ 差し引き 利奈 悪ふざけ リビング\n",
      "\n",
      "Topic #3:\n",
      "メディカル いろいろ 後日 サン 愛嬌 なめらか シトロン クロエ 千尋 実機 入場 旗 スキ 冷 ソフビ 大曲 ガ ワタシプラス 彼岸 ジグ\n",
      "\n",
      "Topic #4:\n",
      "政党 チャクシンガイッケンアリマス 潤子 先ほど キャスト マイナス バツ モニタ りょう 春 におい 春奈 無人 づくし 工具 たこ 商標 チャンピオンシップ ジワッ 今村\n",
      "\n",
      "Topic #5:\n",
      "模倣 サン ルネサンス ナビディスプレイ キムポテサラダ 崇 フィナンシェ 無敵 人妻 モダン ソロ ライブラリ いじり 彼岸 必死 すき 幸子 松嶋 人志 ホン\n",
      "\n",
      "Topic #6:\n",
      "ヒビ くだ 喪服 恋文 山口 ラインナップ 日照 焦点 アスクル エドウィン ビジネス 抹消 バエス キンドルアンリミテッド 太平洋 残念 仕様 イモウト 多分 母体\n",
      "\n",
      "Topic #7:\n",
      "かに デモ 検出 ヒ オランド アップ 政 接 佐川 つきあい 次 パイプライン 変動 寝室 幸恵 水曜 カミナンド 瀬 極右 アディクション\n",
      "\n",
      "Topic #8:\n",
      "イラ 新妻 月曜 民俗 シャンゼリゼ 格 トキメク 南シナ海 ブロゾヴィッチ 峰 ウロコ イラストコラボ グラウンド づくし 慶大 民代 スキ 味付け 妻 そく\n",
      "\n",
      "Topic #9:\n",
      "整形 潘 妊婦 ヤ 島内 合掌 居 尽力 ドコ レコチョク 感動 お前 ピット 改ざん イマデカケルタマ バングル デザイン 意 オリジナリティ 咀嚼\n",
      "\n",
      "Topic #10:\n",
      "むかご デトックス バンコク 健康 塩分 村民 そいつ とうや オカルト 同右 晃 新た 憲 カシオ レイトランチコン カスタム ドラゴンズ 校 半島 ワイプ\n",
      "\n",
      "Topic #11:\n",
      "将来 くん 宮司 品川 様 あさい 消極 愛媛 快挙 ぅ 寺戸 帳 こなし 死後 ざき 嶺 暁子 マッシュポテト 本塁 召使い\n",
      "\n",
      "Topic #12:\n",
      "メディカル 後日 いろいろ プロダクション ダンディ アリストテレス 式典 嶺 徹底 そいつ 大別 アッフル 泊まり 吐水 デンタルケア 年明け 側 ガラスエポキシ やぎ 我が国\n",
      "\n",
      "Topic #13:\n",
      "キス とうさん 大根 タマゴ 場 仏像 在 作品 叩き カエデ 入園 ジャネット コップ 大峰 有効 メディカル 漁港 ステイタス プロジェクションマッピング 健やか\n",
      "\n",
      "Topic #14:\n",
      "ガラスエポキシ 平壌 無実 女性 叩き 来日 更年期 みなさん クライアント 命日 ワンチャンス オフクロ ウッド ボンボン 先制 さえずり 應 把 松浪 レッグプレス\n",
      "\n",
      "Topic #15:\n",
      "帽子 メンテ 南洋 協定 なほ ドリブル アウトロ 否 棚 樹 交友 北 基 エレクトロニクス オレスラソフト 人志 こうえん 割り トレインズ ロセス\n",
      "\n",
      "Topic #16:\n",
      "ソラコム たつお 弔意 徴収 フクス フリット じまん タカ 商戦 ひも 満期 プラスチック くじ引き アルティメイトオメガ がわり ナビディスプレイ スポニチ 師匠 体型 沼津\n",
      "\n",
      "Topic #17:\n",
      "愛嬌 すみれ ソフビ ひばり 偉大 プロダクション ポインタ 大岡 フカヒレエキス スキ そこ ソフトウェア シンクロニシティ 峰 勝機 後日 このほど きゅうり クロエ 就活\n",
      "\n",
      "Topic #18:\n",
      "次号 マック カク 境川 森本 大曲 会合 坪 スタジオジブリ 日本橋 別 レジリエント 廣 毎日 漁港 激変 フッタ やりがい ナビディスプレイ アクティブ\n",
      "\n",
      "Topic #19:\n",
      "土壌 ペン ウルトラ サイゴン 哉 おばさん 果汁 急増 トトロ 所沢 停戦 枠 ロケ テラ ビエ 澪 株分け 大学 エイジ オンライン\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LDAを適用してみよう (ldaパッケージを使った場合)\n",
    "model2 = lda.LDA(n_topics=20, n_iter=1500, random_state=1, alpha=0.5, eta=0.5)\n",
    "model2.fit(A_train)\n",
    "topic_word = model2.topic_word_\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(topic_word):\n",
    "    print('Topic #%d:' % topic_idx)\n",
    "    print(' '.join([all_vocab_ar[i] for i in\n",
    "                    topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-509753.1267447145"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.loglikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: probality: 0.006239\n",
      "Topic #1: probality: 0.009452\n",
      "Topic #2: probality: 0.006699\n",
      "Topic #3: probality: 0.070968\n",
      "Topic #4: probality: 0.004390\n",
      "Topic #5: probality: 0.002607\n",
      "Topic #6: probality: 0.004913\n",
      "Topic #7: probality: 0.011187\n",
      "Topic #8: probality: 0.004820\n",
      "Topic #9: probality: 0.005931\n",
      "Topic #10: probality: 0.005462\n",
      "Topic #11: probality: 0.004817\n",
      "Topic #12: probality: 0.595482\n",
      "Topic #13: probality: 0.008055\n",
      "Topic #14: probality: 0.003563\n",
      "Topic #15: probality: 0.006330\n",
      "Topic #16: probality: 0.005445\n",
      "Topic #17: probality: 0.233463\n",
      "Topic #18: probality: 0.005778\n",
      "Topic #19: probality: 0.004400\n"
     ]
    }
   ],
   "source": [
    "doc_topic_data2 = model2.transform(A_train)\n",
    "for topic_idx, prob in enumerate(doc_topic_data2[0]):\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サイコロ 1面  確率: 0.00\n",
      "サイコロ 2面  確率: 0.18\n",
      "サイコロ 3面  確率: 0.00\n",
      "サイコロ 4面  確率: 0.76\n",
      "サイコロ 5面  確率: 0.03\n",
      "サイコロ 6面  確率: 0.03\n"
     ]
    }
   ],
   "source": [
    "# （参考）ディリクレ分布の挙動\n",
    "alpha = 0.1\n",
    "K = 6\n",
    "sampled_probs = np.random.dirichlet([alpha for i in range(K)])\n",
    "for i, prob in enumerate(sampled_probs):\n",
    "    print('サイコロ %d面  確率: %.2f'%(i+1, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
